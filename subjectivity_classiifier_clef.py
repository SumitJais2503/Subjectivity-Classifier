# -*- coding: utf-8 -*-
"""Subjectivity Classiifier- Clef.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13YguaDz6z-e3qWcZbNJW_VauzbQ7aW3D
"""

pip install datasets

"""Imports

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch
from torch.utils.data import DataLoader

from datasets import Dataset

from sklearn.metrics import classification_report,confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""load dataset"""

# Load the data (TSV = tab-separated values)
langs = ['ar', 'bg', 'de', 'en', 'it']
dfs = []

for lang in langs:
    # Use f-string formatting to correctly insert the 'lang' value into the filename
    df = pd.read_csv(f"/content/train_{lang}.tsv", sep="\t")
    df['lang'] = lang
    dfs.append(df)

data = pd.concat(dfs, ignore_index=True)

# # Only drop 'Unnamed: 0' if it exists
# if 'Unnamed: 0' in data.columns:
#     data = data.drop(columns=['Unnamed: 0'])

# # Drop any rows with missing values
# data.dropna(inplace=True)

# Display the dataframe
data

data = data.sample(n=2500, random_state=42).reset_index(drop=True)
data

"""preprocessing"""

import re
import nltk
from nltk.corpus import stopwords

# Download necessary NLTK resources
nltk.download('stopwords')

# Manually define Bulgarian stopwords
bulgarian_stopwords = {
    'Ğ°Ğ·', 'Ğ°ĞºĞ¾', 'Ğ°Ğ»Ğ°', 'Ğ±ĞµĞ·', 'Ğ±ĞµÑˆĞµ', 'Ğ±Ğ¸', 'Ğ±Ğ¸Ğ»', 'Ğ±Ğ¸Ğ»Ğ°', 'Ğ±Ğ¸Ğ»Ğ¸', 'Ğ±Ğ¸Ğ»Ğ¾', 'Ğ±Ğ¸Ñ…',
    'Ğ±Ğ¸Ñ…Ğ°', 'Ğ±Ğ¸Ñ…Ğ¼Ğµ', 'Ğ±Ğ¸Ñ…Ñ‚Ğµ', 'Ğ±ÑÑ…Ğ°', 'Ğ±ÑŠĞ´Ğ°Ñ‚', 'Ğ±ÑŠĞ´Ğµ', 'Ğ±ÑŠĞ´ĞµĞ¼', 'Ğ±ÑŠĞ´ĞµÑ‚Ğµ', 'Ğ±ÑŠĞ´ĞµĞ¹ĞºĞ¸', 'Ğ±ÑŠĞ´Ğ°',
    'Ğ²', 'Ğ²Ğ°Ñ', 'Ğ²Ğ°Ñˆ', 'Ğ²Ğ°ÑˆĞ°', 'Ğ²Ğ°ÑˆĞµ', 'Ğ²Ğ°ÑˆĞ¸', 'Ğ²Ğ¸Ğµ', 'Ğ²ÑŠĞ²', 'Ğ²ÑŠĞ¿Ñ€ĞµĞºĞ¸', 'Ğ²ÑŠÑ€Ñ…Ñƒ', 'Ğ³Ğ¸', 'Ğ³Ğ¾', 'Ğ³Ğ¾Ğ²Ğ°',
    'Ğ´Ğ°Ğ»Ğ¸', 'Ğ´Ğ°', 'Ğ´Ğ¾', 'Ğ´Ğ¾Ñ€Ğ¸', 'Ğ´Ñ€ÑƒĞ³', 'Ğ´Ñ€ÑƒĞ³Ğ°', 'Ğ´Ñ€ÑƒĞ³Ğ¸', 'Ğ´Ñ€ÑƒĞ³Ğ¾', 'Ğµ', 'ĞµĞ²ĞµĞ½Ñ‚ÑƒĞ°Ğ»Ğ½Ğ¾', 'ĞµĞ´Ğ²Ğ°', 'ĞµĞ´Ğ²Ğ° Ğ»Ğ¸',
    'ĞµĞ´Ğ¸Ğ½', 'ĞµĞ´Ğ½Ğ°', 'ĞµĞ´Ğ½Ğ¾', 'ĞµĞ´Ğ½Ğ¸', 'ĞµÑ‚Ğ¾', 'Ğ·Ğ°', 'Ğ·Ğ°Ğ´', 'Ğ·Ğ°ĞµĞ´Ğ½Ğ¾', 'Ğ·Ğ°Ñ‰Ğ¾', 'Ğ·Ğ°Ñ‰Ğ¾Ñ‚Ğ¾', 'Ğ¸', 'Ğ¸Ğ·', 'Ğ¸Ğ»Ğ¸',
    'Ğ¸Ğ¼', 'Ğ¸Ğ¼Ğ°', 'Ğ¸Ğ¼Ğ°Ñ‚', 'Ğ¸ÑĞºĞ°', 'Ğ¸ÑĞºĞ°Ñ‚', 'ĞºĞ°Ğº', 'ĞºĞ°ĞºĞ²Ğ¾', 'ĞºĞ°ĞºÑ‚Ğ¾', 'ĞºĞ°Ñ‚Ğ¾', 'ĞºĞ¾Ğ³Ğ°', 'ĞºĞ¾Ğ³Ğ°Ñ‚Ğ¾', 'ĞºĞ¾Ğ¹',
    'ĞºĞ¾Ğ¹Ñ‚Ğ¾', 'ĞºĞ¾ÑÑ‚Ğ¾', 'ĞºĞ¾Ğµ', 'ĞºĞ¾Ğ¸', 'ĞºÑŠĞ´Ğµ', 'ĞºÑŠĞ´ĞµÑ‚Ğ¾', 'Ğ»Ğ¸', 'Ğ¼Ğ°Ğ»ĞºĞ¾', 'Ğ¼Ğµ', 'Ğ¼ĞµĞ¶Ğ´Ñƒ', 'Ğ¼ĞµĞ½', 'Ğ¼Ğ¸', 'Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ¸Ñ',
    'Ğ¼Ğ¸Ğ½Ğ°', 'Ğ¼Ğ½Ğ¾Ğ³Ğ¾', 'Ğ¼Ğ¾Ğ³Ğ°Ñ‚', 'Ğ¼Ğ¾Ğ¶Ğµ', 'Ğ¼Ğ¾Ğ¶ĞµÑˆĞµ', 'Ğ¼Ğ¾ĞµÑ‚Ğ¾', 'Ğ¼Ğ¾Ğ¹', 'Ğ¼Ğ¾Ñ', 'Ğ¼Ğ¾ÑÑ‚Ğ°', 'Ğ¼Ñƒ', 'Ğ½', 'Ğ½Ğ°',
    'Ğ½Ğ°Ğ´', 'Ğ½Ğ°Ğ¹', 'Ğ½Ğ°ĞºÑ€Ğ°Ñ', 'Ğ½Ğ°Ğ¼', 'Ğ½Ğ°Ñ', 'Ğ½Ğµ', 'Ğ½ĞµĞ³Ğ¾', 'Ğ½ĞµĞ¹', 'Ğ½ĞµÑ', 'Ğ½ĞµÑ‰Ğ°', 'Ğ½Ğ¸', 'Ğ½Ğ¸Ğµ', 'Ğ½Ğ¸Ğ¼',
    'Ğ½Ğ¸Ñ…', 'Ğ½Ğ¾', 'Ğ¾Ğ±Ğ°Ñ‡Ğµ', 'Ğ¾Ñ‚', 'Ğ¾Ñ‚Ğ³Ğ¾Ñ€Ğµ', 'Ğ¾Ñ‚Ğ½Ğ¾ÑĞ½Ğ¾', 'Ğ¿Ğ¾', 'Ğ¿Ğ¾Ğ´', 'Ğ¿Ğ¾ĞºÑ€Ğ°Ğ¹', 'Ğ¿Ğ¾Ğ½Ğµ', 'Ğ¿Ğ¾Ñ€Ğ°Ğ´Ğ¸', 'Ğ¿Ğ¾ÑĞ»Ğµ',
    'Ğ¿Ñ€ĞµĞ´', 'Ğ¿Ñ€ĞµĞ´Ğ¸', 'Ğ¿Ñ€ĞµĞ·', 'Ğ¿Ñ€Ğ¸', 'Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²', 'Ğ¿ÑŠĞº', 'ÑĞ°Ğ¼', 'ÑĞ°Ğ¼Ğ°', 'ÑĞ°Ğ¼Ğ¸', 'ÑĞ°Ğ¼Ğ¾', 'ÑĞ°Ğ¼Ğ°Ñ‚Ğ°', 'ÑĞ°Ğ¼Ğ¾Ñ‚Ğ¾',
    'ÑĞµ', 'ÑĞµĞ³Ğ°', 'ÑĞ¸', 'ÑĞºĞ¾Ñ€Ğ¾', 'ÑĞ»ĞµĞ´', 'ÑĞ¼Ğµ', 'ÑĞ¼ÑÑ…', 'ÑÑŠĞ¼', 'ÑÑŠÑ', 'ÑÑŠÑ‰Ğ¾', 'ÑÑŠÑ‰Ğ¸ÑÑ‚', 'ÑÑŠÑ‰Ğ°Ñ‚Ğ°', 'ÑÑŠÑ‰Ğ¾Ñ‚Ğ¾',
    'ÑÑŠÑ‰ĞµÑÑ‚Ğ²ÑƒĞ²Ğ°', 'Ñ‚Ğ°', 'Ñ‚Ğ°Ğ·Ğ¸', 'Ñ‚Ğ°ĞºĞ°', 'Ñ‚Ğ°ĞºÑŠĞ²', 'Ñ‚Ğ°Ğ¼', 'Ñ‚Ğµ', 'Ñ‚ĞµĞ±', 'Ñ‚ĞµĞ±Ğµ', 'Ñ‚ĞµĞ·Ğ¸', 'Ñ‚Ğ¸', 'Ñ‚Ğ¾Ğ²Ğ°', 'Ñ‚Ğ¾Ğ³Ğ°Ğ²Ğ°',
    'Ñ‚Ğ¾Ğ¹', 'Ñ‚Ğ¾Ğ»ĞºĞ¾Ğ²Ğ°', 'Ñ‚Ğ¾Ğ·Ğ¸', 'Ñ‚Ğ¾', 'Ñ‚Ğ¾Ñ‡Ğ½Ğ¾', 'Ñ‚Ñ€ÑĞ±Ğ²Ğ°', 'Ñ‚Ñ', 'Ñ‚ÑÑ…', 'Ñƒ', 'Ñ…ÑƒĞ±Ğ°Ğ²Ğ¾', 'Ñ‡Ñ€ĞµĞ·', 'Ñ‡Ğµ', 'Ñ‡ĞµÑÑ‚Ğ¾',
    'Ñ‰Ğµ', 'Ñ‰Ğ¾Ğ¼', 'Ñ'
}

# Map language codes to their stopwords
stopword_map = {
    'ar': set(stopwords.words('arabic')),
    'bg': bulgarian_stopwords,  # use manual stopwords for Bulgarian
    'de': set(stopwords.words('german')),
    'en': set(stopwords.words('english')),
    'it': set(stopwords.words('italian')),
}

# Function to clean a single statement
def clean_statement(statement, lang):
    if not isinstance(statement, str):
        return statement  # In case some value is NaN or not a string

    # Lowercase
    statement = statement.lower()

    # Remove special characters and numbers
    statement = re.sub(r'[^\w\s]', '', statement)
    statement = re.sub(r'\d+', '', statement)

    # Tokenize and remove stopwords
    words = statement.split()
    stop_words = stopword_map.get(lang, set())
    words = [word for word in words if word not in stop_words]

    # Rejoin
    return ' '.join(words)

# Apply the cleaning function
data['sentence'] = data.apply(lambda row: clean_statement(row['sentence'], row['lang']), axis=1)

data

"""BALANCE DATASETS"""

data['label'].value_counts()

from imblearn.over_sampling import RandomOverSampler
import pandas as pd

# Create an instance of RandomOverSampler
ros = RandomOverSampler(sampling_strategy='auto', random_state=42)

# Define features (X) and target (y)
X = data.drop(columns=['label'])  # Change 'status' â†’ 'label'
y = data['label']

# Perform oversampling
X_resampled, y_resampled = ros.fit_resample(X, y)

# Combine the oversampled features and target back into a DataFrame
data = pd.concat([X_resampled, y_resampled], axis=1)

# Check the new class distribution
print(data['label'].value_counts())

# Label encoding for 'status' column
label_encoder = LabelEncoder()
data['label'] = label_encoder.fit_transform(data['label'])

from sklearn.model_selection import train_test_split

train_texts, test_texts, train_labels, test_labels = train_test_split(
    data['sentence'], data['label'], test_size=0.2, random_state=42
)

max([len(text) for text in data['sentence']])

from transformers import BertTokenizer

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

# Tokenize the train and test texts
train_encodings = tokenizer(list(train_texts), padding=True, truncation=True, max_length=200)
test_encodings = tokenizer(list(test_texts), padding=True, truncation=True, max_length=200)

from datasets import Dataset

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': train_labels.tolist()
})

test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': test_labels.tolist()
})

pip install torch

pip install huggingface_hub[hf_xet]

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained(
    "bert-base-multilingual-cased",
    num_labels=len(label_encoder.classes_)
)

from transformers import BertForSequenceClassification, Trainer, TrainingArguments

import os

# ğŸš« Disable W&B logging
os.environ["WANDB_DISABLED"] = "true"

# ğŸ§  Load model
model = BertForSequenceClassification.from_pretrained(
    "bert-base-multilingual-cased",
    num_labels=len(label_encoder.classes_)
)

# âš™ï¸ Set training arguments
training_args = TrainingArguments(
    output_dir="./results",              # Output directory for results
    eval_strategy="epoch",               # âœ… updated to avoid deprecation
    save_strategy="epoch",               # Save model at end of each epoch
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=7,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    lr_scheduler_type="linear",
    warmup_steps=500,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    save_total_limit=3,
    gradient_accumulation_steps=2
)

# ğŸ‹ï¸ Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# ğŸš€ Train the model
trainer.train()



# ğŸ“Š 1. Make predictions
predictions, labels, _ = trainer.predict(test_dataset)

# ğŸ¯ 2. Convert logits to predicted class labels
predicted_labels = np.argmax(predictions, axis=1)

# ğŸ§¾ 3. Classification Report
print("Classification Report:")
print(classification_report(test_labels, predicted_labels, target_names=label_encoder.classes_))

# ğŸ”€ 4. Confusion Matrix
cm = confusion_matrix(test_labels, predicted_labels)

plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)

plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# âœ… Save model & tokenizer
trainer.save_model("saved_subjectivity_Classifier_bert")           # Save fine-tuned BERT model
tokenizer.save_pretrained("saved_subjectivity_Classifier_bert")    # Save tokenizer

# âœ… Save LabelEncoder
import pickle
with open("label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import pickle

# Load the model and tokenizer from the saved directory
model = AutoModelForSequenceClassification.from_pretrained("saved_subjectivity_Classifier_bert")
tokenizer = AutoTokenizer.from_pretrained("saved_subjectivity_Classifier_bert")

# Load the LabelEncoder
with open('label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)

import torch

# ğŸ§¼ 1. Detection Function
def detect_label(text, lang):
    # Clean the input
    cleaned_text = clean_statement(text, lang)

    # Tokenize the input
    inputs = tokenizer(
        cleaned_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )

    # Run the model
    with torch.no_grad():
        outputs = model(**inputs)

    # Get predicted class
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()

    # Decode predicted label
    return label_encoder.inverse_transform([predicted_class])[0]

# ğŸ§ª 2. Sample Texts with Language
sample_texts = [
    ("Ø£Ø´Ø¹Ø± Ø£Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© Ø£ØµØ¨Ø­Øª Ø£ÙƒØ«Ø± Ø§Ø²Ø¯Ø­Ø§Ù…Ù‹Ø§ Ù…Ù…Ø§ ÙƒØ§Ù†Øª Ø¹Ù„ÙŠÙ‡.", 'ar'),
    ("ØªØ£Ø³Ø³Øª Ø´Ø±ÙƒØ© Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø¹Ø§Ù… 1933.", 'ar'),
    ("Ø¨ØµØ±Ø§Ø­Ø©ØŒ Ø£Ø¹ØªÙ‚Ø¯ Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ø­ÙŠ Ù‡Ùˆ Ø§Ù„Ø£Ø¬Ù…Ù„ ÙÙŠ Ø§Ù„Ø±ÙŠØ§Ø¶.", 'ar'),
    ("Ø£Ø¹Ù„Ù†Øª Ø§Ù„Ø­ÙƒÙˆÙ…Ø© Ø±ÙØ¹ Ø£Ø³Ø¹Ø§Ø± Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡ Ø¨Ù†Ø³Ø¨Ø© 10Ùª.", 'ar'),
    ("ÙÙŠ Ø±Ø£ÙŠÙŠØŒ ÙŠØ¬Ø¨ ØªØ­Ø³ÙŠÙ† ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„Ù†Ù‚Ù„ Ø§Ù„Ø¹Ø§Ù….", 'ar'),
    ("Ø§Ù†ØªÙ‡Ù‰ Ù…Ø´Ø±ÙˆØ¹ ØªÙˆØ³Ø¹Ø© Ø§Ù„Ù…Ø·Ø§Ø± Ø®Ù„Ø§Ù„ Ø¹Ø§Ù… 2022.", 'ar'),
    ("Ø£Ø¸Ù† Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„ÙØ±ÙŠÙ‚ Ù„Ø¯ÙŠÙ‡ ÙØ±ØµØ© ÙƒØ¨ÙŠØ±Ø© Ù„Ù„ÙÙˆØ².", 'ar'),
    ("Ø¹Ø¯Ø¯ Ø³ÙƒØ§Ù† Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø¨Ù„Øº 36 Ù…Ù„ÙŠÙˆÙ† Ù†Ø³Ù…Ø© Ø¹Ø§Ù… 2024.", 'ar'),
    ("Ø£Ø­Ø¨Ø¨Øª Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø­Ø§Ø¶Ø±Ø© Ø¨Ø´ÙƒÙ„ Ø¨Ø³ÙŠØ· ÙˆÙˆØ§Ø¶Ø­.", 'ar'),
    ("Ù‡Ø·Ù„Øª Ø£Ù…Ø·Ø§Ø± ØºØ²ÙŠØ±Ø© Ø¹Ù„Ù‰ Ø¬Ø¯Ø© Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„Ù…Ø§Ø¶ÙŠ.", 'ar'),

    ("Credo che questa sia stata la scelta migliore.", 'it'),
    ("Il museo nazionale Ã¨ stato fondato nel 1865.", 'it'),
    ("Secondo me, il nuovo ristorante offre piatti deliziosi.", 'it'),
    ("La temperatura a Roma oggi Ã¨ di 27 gradi.", 'it'),
    ("Penso che dovrebbero migliorare il servizio clienti.", 'it'),
    ("L'azienda ha registrato una crescita del 12% quest'anno.", 'it'),
    ("A mio avviso, il film era troppo lungo e noioso.", 'it'),
    ("Il treno delle 10:00 per Milano Ã¨ stato cancellato.", 'it'),
    ("Mi sembra che questa decisione sia affrettata.", 'it'),
    ("Il concerto inizia alle 20:30 al teatro centrale.", 'it'),

    ("ĞœĞ¸ÑĞ»Ñ, Ñ‡Ğµ Ñ‚Ğ¾Ğ·Ğ¸ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ñ‰Ğµ Ğ¸Ğ¼Ğ° Ğ³Ğ¾Ğ»ÑĞ¼ ÑƒÑĞ¿ĞµÑ….", 'bg'),
    ("ĞœĞ¸Ğ½Ğ¸ÑÑ‚ĞµÑ€ÑÑ‚Ğ²Ğ¾Ñ‚Ğ¾ Ğ½Ğ° Ğ·Ğ´Ñ€Ğ°Ğ²ĞµĞ¾Ğ¿Ğ°Ğ·Ğ²Ğ°Ğ½ĞµÑ‚Ğ¾ Ğ¾Ğ±ÑĞ²Ğ¸ Ğ½Ğ¾Ğ²Ğ° ĞºĞ°Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ.", 'bg'),
    ("ĞÑ‚ Ğ¼Ğ¾Ñ Ğ³Ğ»ĞµĞ´Ğ½Ğ° Ñ‚Ğ¾Ñ‡ĞºĞ°, Ğ¼ĞµÑ€ĞºĞ¸Ñ‚Ğµ ÑĞ° Ğ¿Ñ€ĞµĞºĞ°Ğ»ĞµĞ½Ğ¾ ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸.", 'bg'),
    ("Ğ¢ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ¸Ñ‚Ğµ Ñ‰Ğµ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½Ğ°Ñ‚ 35 Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ° Ñ‚Ğ°Ğ·Ğ¸ ÑĞµĞ´Ğ¼Ğ¸Ñ†Ğ°.", 'bg'),
    ("Ğ’ÑÑ€Ğ²Ğ°Ğ¼, Ñ‡Ğµ Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ´Ğ° Ğ¿Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ĞµĞ¼ Ğ¾Ñ‰Ğµ Ğ¿Ğ¾Ğ²ĞµÑ‡Ğµ.", 'bg'),
    ("ĞĞ¾Ğ²Ğ¸ÑÑ‚ Ğ¼Ğ¾ÑÑ‚ Ñ‰Ğµ Ğ±ÑŠĞ´Ğµ Ğ·Ğ°Ğ²ÑŠÑ€ÑˆĞµĞ½ Ğ´Ğ¾ ĞºÑ€Ğ°Ñ Ğ½Ğ° Ğ³Ğ¾Ğ´Ğ¸Ğ½Ğ°Ñ‚Ğ°.", 'bg'),
    ("Ğ¡Ğ¼ÑÑ‚Ğ°Ğ¼, Ñ‡Ğµ Ñ‚Ñ€ÑĞ±Ğ²Ğ° Ğ´Ğ° ÑĞµ Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ€Ğ° Ğ¿Ğ¾Ğ²ĞµÑ‡Ğµ Ğ² Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµÑ‚Ğ¾.", 'bg'),
    ("ĞĞĞ¡Ğ Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ° Ğ½Ğ¾Ğ²Ğ° Ğ¼Ğ¸ÑĞ¸Ñ ĞºÑŠĞ¼ ĞœĞ°Ñ€Ñ Ğ¿Ñ€ĞµĞ· 2026 Ğ³Ğ¾Ğ´Ğ¸Ğ½Ğ°.", 'bg'),
    ("Ğ§ĞµÑÑ‚Ğ½Ğ¾ ĞºĞ°Ğ·Ğ°Ğ½Ğ¾, Ñ‚Ğ¾Ğ²Ğ° Ğµ Ğ½Ğ°Ğ¹-Ğ´Ğ¾Ğ±Ñ€Ğ¸ÑÑ‚ Ğ¾Ñ‚Ğ±Ğ¾Ñ€, ĞºĞ¾Ğ¹Ñ‚Ğ¾ ÑÑŠĞ¼ Ğ²Ğ¸Ğ¶Ğ´Ğ°Ğ».", 'bg'),
    ("Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€Ğ¸Ñ Ñ‰Ğµ Ğ±ÑŠĞ´Ğµ Ğ´Ğ¾Ğ¼Ğ°ĞºĞ¸Ğ½ Ğ½Ğ° Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´ĞµĞ½ Ñ„Ğ¾Ñ€ÑƒĞ¼ Ñ‚Ğ°Ğ·Ğ¸ ĞµÑĞµĞ½.", 'bg'),

    ("I honestly believe this city is the best in the country.", 'en'),
    ("The bridge collapsed during a storm last year.", 'en'),
    ("In my opinion, the new rules are too strict.", 'en'),
    ("The hospital admitted over 300 patients last week.", 'en'),
    ("I feel this movie deserved more awards.", 'en'),
    ("The team won the championship by 5 points.", 'en'),
    ("It's clear to me that the project is a failure.", 'en'),
    ("The company reported a 7% increase in profits.", 'en'),
    ("I absolutely loved the performance last night.", 'en'),
    ("The annual conference will be held in Chicago this year.", 'en'),

    ("Meiner Meinung nach ist dieses Buch wirklich inspirierend.", 'de'),
    ("Das Museum wurde im Jahr 1902 erÃ¶ffnet.", 'de'),
    ("Ich finde, dass die neuen Regeln unfair sind.", 'de'),
    ("Die Temperaturen erreichten gestern 30 Grad Celsius.", 'de'),
    ("Ich glaube, dass wir mehr in Bildung investieren sollten.", 'de'),
    ("Das Unternehmen erÃ¶ffnete eine neue Filiale in Hamburg.", 'de'),
    ("Aus meiner Sicht war das Konzert ein voller Erfolg.", 'de'),
    ("Die Bahnstrecke wurde wegen Wartungsarbeiten gesperrt.", 'de'),
    ("Ich denke, der neue Film ist absolut sehenswert.", 'de'),
    ("Die Stadtbibliothek wird im nÃ¤chsten Monat renoviert.", 'de')
]

# ğŸ” 3. Run detection
for text, lang in sample_texts:
    predicted_class = detect_label(text, lang)
    print(f"ğŸ“ Sentence: {text}\nâ¡ï¸ Predicted Label: {predicted_class}\n")

import pandas as pd
import torch
import zipfile
# import re
# import nltk
# from nltk.corpus import stopwords

# ğŸ§¼ 1. Detection Function



# # Download NLTK stopwords
# nltk.download('stopwords')

# # Get English stopwords from NLTK
# stop_words = set(stopwords.words('romanian'))

# def clean_statement(statement):
#     # Convert to lowercase
#     statement = statement.lower()

#     # Remove special characters (punctuation, non-alphabetic characters)
#     statement = re.sub(r'[^\w\s]', '', statement)

#     # Remove numbers
#     statement = re.sub(r'\d+', '', statement)

#     # Tokenize and remove stopwords
#     words = statement.split()
#     words = [word for word in words if word not in stop_words]

#     # Rejoin into a cleaned statement
#     cleaned_statement = ' '.join(words)
#     return cleaned_statement

# Apply to the correct column: 'sentence'


def detect_label(text):
    # cleaned_text = clean_statement(text)

    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()

    return label_encoder.inverse_transform([predicted_class])[0]

# ğŸ“‚ 2. Load the input TSV file
input_file = "/content/test_pol_unlabeled.tsv"
df = pd.read_csv(input_file, sep="\t")

assert 'sentence_id' in df.columns and 'sentence' in df.columns, "Input file must have 'sentence_id' and 'sentence' columns"

# ğŸ” 3. Predict labels and keep alignment
labels = df['sentence'].map(detect_label)  # map() keeps index alignment

# Combine sentence_id + label
output_df = pd.DataFrame({
    'sentence_id': df['sentence_id'],
    'label': labels
})

# ğŸ’¾ 4. Save the TSV output
tsv_output_path = "/content/task1_test_pol.tsv"
output_df.to_csv(tsv_output_path, sep="\t", index=False)

# ğŸ—œï¸ 5. Zip the TSV file
zip_output_path = "/content/task1_test_pol.zip"
with zipfile.ZipFile(zip_output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
    zipf.write(tsv_output_path, arcname="task1_test_pol.tsv")

print(f"âœ… TSV saved to: {tsv_output_path}")
print(f"âœ… ZIP file created: {zip_output_path}")