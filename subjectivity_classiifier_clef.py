# -*- coding: utf-8 -*-
"""Subjectivity Classiifier- Clef.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13YguaDz6z-e3qWcZbNJW_VauzbQ7aW3D
"""

pip install datasets

"""Imports

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
import torch
from torch.utils.data import DataLoader

from datasets import Dataset

from sklearn.metrics import classification_report,confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""load dataset"""

# Load the data (TSV = tab-separated values)
langs = ['ar', 'bg', 'de', 'en', 'it']
dfs = []

for lang in langs:
    # Use f-string formatting to correctly insert the 'lang' value into the filename
    df = pd.read_csv(f"/content/train_{lang}.tsv", sep="\t")
    df['lang'] = lang
    dfs.append(df)

data = pd.concat(dfs, ignore_index=True)

# # Only drop 'Unnamed: 0' if it exists
# if 'Unnamed: 0' in data.columns:
#     data = data.drop(columns=['Unnamed: 0'])

# # Drop any rows with missing values
# data.dropna(inplace=True)

# Display the dataframe
data

data = data.sample(n=2500, random_state=42).reset_index(drop=True)
data

"""preprocessing"""

import re
import nltk
from nltk.corpus import stopwords

# Download necessary NLTK resources
nltk.download('stopwords')

# Manually define Bulgarian stopwords
bulgarian_stopwords = {
    '–∞–∑', '–∞–∫–æ', '–∞–ª–∞', '–±–µ–∑', '–±–µ—à–µ', '–±–∏', '–±–∏–ª', '–±–∏–ª–∞', '–±–∏–ª–∏', '–±–∏–ª–æ', '–±–∏—Ö',
    '–±–∏—Ö–∞', '–±–∏—Ö–º–µ', '–±–∏—Ö—Ç–µ', '–±—è—Ö–∞', '–±—ä–¥–∞—Ç', '–±—ä–¥–µ', '–±—ä–¥–µ–º', '–±—ä–¥–µ—Ç–µ', '–±—ä–¥–µ–π–∫–∏', '–±—ä–¥–∞',
    '–≤', '–≤–∞—Å', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–∏', '–≤–∏–µ', '–≤—ä–≤', '–≤—ä–ø—Ä–µ–∫–∏', '–≤—ä—Ä—Ö—É', '–≥–∏', '–≥–æ', '–≥–æ–≤–∞',
    '–¥–∞–ª–∏', '–¥–∞', '–¥–æ', '–¥–æ—Ä–∏', '–¥—Ä—É–≥', '–¥—Ä—É–≥–∞', '–¥—Ä—É–≥–∏', '–¥—Ä—É–≥–æ', '–µ', '–µ–≤–µ–Ω—Ç—É–∞–ª–Ω–æ', '–µ–¥–≤–∞', '–µ–¥–≤–∞ –ª–∏',
    '–µ–¥–∏–Ω', '–µ–¥–Ω–∞', '–µ–¥–Ω–æ', '–µ–¥–Ω–∏', '–µ—Ç–æ', '–∑–∞', '–∑–∞–¥', '–∑–∞–µ–¥–Ω–æ', '–∑–∞—â–æ', '–∑–∞—â–æ—Ç–æ', '–∏', '–∏–∑', '–∏–ª–∏',
    '–∏–º', '–∏–º–∞', '–∏–º–∞—Ç', '–∏—Å–∫–∞', '–∏—Å–∫–∞—Ç', '–∫–∞–∫', '–∫–∞–∫–≤–æ', '–∫–∞–∫—Ç–æ', '–∫–∞—Ç–æ', '–∫–æ–≥–∞', '–∫–æ–≥–∞—Ç–æ', '–∫–æ–π',
    '–∫–æ–π—Ç–æ', '–∫–æ—è—Ç–æ', '–∫–æ–µ', '–∫–æ–∏', '–∫—ä–¥–µ', '–∫—ä–¥–µ—Ç–æ', '–ª–∏', '–º–∞–ª–∫–æ', '–º–µ', '–º–µ–∂–¥—É', '–º–µ–Ω', '–º–∏', '–º–∏–Ω–∞–ª–∏—è',
    '–º–∏–Ω–∞', '–º–Ω–æ–≥–æ', '–º–æ–≥–∞—Ç', '–º–æ–∂–µ', '–º–æ–∂–µ—à–µ', '–º–æ–µ—Ç–æ', '–º–æ–π', '–º–æ—è', '–º–æ—è—Ç–∞', '–º—É', '–Ω', '–Ω–∞',
    '–Ω–∞–¥', '–Ω–∞–π', '–Ω–∞–∫—Ä–∞—è', '–Ω–∞–º', '–Ω–∞—Å', '–Ω–µ', '–Ω–µ–≥–æ', '–Ω–µ–π', '–Ω–µ—è', '–Ω–µ—â–∞', '–Ω–∏', '–Ω–∏–µ', '–Ω–∏–º',
    '–Ω–∏—Ö', '–Ω–æ', '–æ–±–∞—á–µ', '–æ—Ç', '–æ—Ç–≥–æ—Ä–µ', '–æ—Ç–Ω–æ—Å–Ω–æ', '–ø–æ', '–ø–æ–¥', '–ø–æ–∫—Ä–∞–π', '–ø–æ–Ω–µ', '–ø–æ—Ä–∞–¥–∏', '–ø–æ—Å–ª–µ',
    '–ø—Ä–µ–¥', '–ø—Ä–µ–¥–∏', '–ø—Ä–µ–∑', '–ø—Ä–∏', '–ø—Ä–æ—Ç–∏–≤', '–ø—ä–∫', '—Å–∞–º', '—Å–∞–º–∞', '—Å–∞–º–∏', '—Å–∞–º–æ', '—Å–∞–º–∞—Ç–∞', '—Å–∞–º–æ—Ç–æ',
    '—Å–µ', '—Å–µ–≥–∞', '—Å–∏', '—Å–∫–æ—Ä–æ', '—Å–ª–µ–¥', '—Å–º–µ', '—Å–º—è—Ö', '—Å—ä–º', '—Å—ä—Å', '—Å—ä—â–æ', '—Å—ä—â–∏—è—Ç', '—Å—ä—â–∞—Ç–∞', '—Å—ä—â–æ—Ç–æ',
    '—Å—ä—â–µ—Å—Ç–≤—É–≤–∞', '—Ç–∞', '—Ç–∞–∑–∏', '—Ç–∞–∫–∞', '—Ç–∞–∫—ä–≤', '—Ç–∞–º', '—Ç–µ', '—Ç–µ–±', '—Ç–µ–±–µ', '—Ç–µ–∑–∏', '—Ç–∏', '—Ç–æ–≤–∞', '—Ç–æ–≥–∞–≤–∞',
    '—Ç–æ–π', '—Ç–æ–ª–∫–æ–≤–∞', '—Ç–æ–∑–∏', '—Ç–æ', '—Ç–æ—á–Ω–æ', '—Ç—Ä—è–±–≤–∞', '—Ç—è', '—Ç—è—Ö', '—É', '—Ö—É–±–∞–≤–æ', '—á—Ä–µ–∑', '—á–µ', '—á–µ—Å—Ç–æ',
    '—â–µ', '—â–æ–º', '—è'
}

# Map language codes to their stopwords
stopword_map = {
    'ar': set(stopwords.words('arabic')),
    'bg': bulgarian_stopwords,  # use manual stopwords for Bulgarian
    'de': set(stopwords.words('german')),
    'en': set(stopwords.words('english')),
    'it': set(stopwords.words('italian')),
}

# Function to clean a single statement
def clean_statement(statement, lang):
    if not isinstance(statement, str):
        return statement  # In case some value is NaN or not a string

    # Lowercase
    statement = statement.lower()

    # Remove special characters and numbers
    statement = re.sub(r'[^\w\s]', '', statement)
    statement = re.sub(r'\d+', '', statement)

    # Tokenize and remove stopwords
    words = statement.split()
    stop_words = stopword_map.get(lang, set())
    words = [word for word in words if word not in stop_words]

    # Rejoin
    return ' '.join(words)

# Apply the cleaning function
data['sentence'] = data.apply(lambda row: clean_statement(row['sentence'], row['lang']), axis=1)

data

"""BALANCE DATASETS"""

data['label'].value_counts()

from imblearn.over_sampling import RandomOverSampler
import pandas as pd

# Create an instance of RandomOverSampler
ros = RandomOverSampler(sampling_strategy='auto', random_state=42)

# Define features (X) and target (y)
X = data.drop(columns=['label'])  # Change 'status' ‚Üí 'label'
y = data['label']

# Perform oversampling
X_resampled, y_resampled = ros.fit_resample(X, y)

# Combine the oversampled features and target back into a DataFrame
data = pd.concat([X_resampled, y_resampled], axis=1)

# Check the new class distribution
print(data['label'].value_counts())

# Label encoding for 'status' column
label_encoder = LabelEncoder()
data['label'] = label_encoder.fit_transform(data['label'])

from sklearn.model_selection import train_test_split

train_texts, test_texts, train_labels, test_labels = train_test_split(
    data['sentence'], data['label'], test_size=0.2, random_state=42
)

max([len(text) for text in data['sentence']])

from transformers import BertTokenizer

# Load tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

# Tokenize the train and test texts
train_encodings = tokenizer(list(train_texts), padding=True, truncation=True, max_length=200)
test_encodings = tokenizer(list(test_texts), padding=True, truncation=True, max_length=200)

from datasets import Dataset

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': train_labels.tolist()
})

test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': test_labels.tolist()
})

pip install torch

pip install huggingface_hub[hf_xet]

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained(
    "bert-base-multilingual-cased",
    num_labels=len(label_encoder.classes_)
)

from transformers import BertForSequenceClassification, Trainer, TrainingArguments

import os

# üö´ Disable W&B logging
os.environ["WANDB_DISABLED"] = "true"

# üß† Load model
model = BertForSequenceClassification.from_pretrained(
    "bert-base-multilingual-cased",
    num_labels=len(label_encoder.classes_)
)

# ‚öôÔ∏è Set training arguments
training_args = TrainingArguments(
    output_dir="./results",              # Output directory for results
    eval_strategy="epoch",               # ‚úÖ updated to avoid deprecation
    save_strategy="epoch",               # Save model at end of each epoch
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=7,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    lr_scheduler_type="linear",
    warmup_steps=500,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    save_total_limit=3,
    gradient_accumulation_steps=2
)

# üèãÔ∏è Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# üöÄ Train the model
trainer.train()



# üìä 1. Make predictions
predictions, labels, _ = trainer.predict(test_dataset)

# üéØ 2. Convert logits to predicted class labels
predicted_labels = np.argmax(predictions, axis=1)

# üßæ 3. Classification Report
print("Classification Report:")
print(classification_report(test_labels, predicted_labels, target_names=label_encoder.classes_))

# üîÄ 4. Confusion Matrix
cm = confusion_matrix(test_labels, predicted_labels)

plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)

plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# ‚úÖ Save model & tokenizer
trainer.save_model("saved_subjectivity_Classifier_bert")           # Save fine-tuned BERT model
tokenizer.save_pretrained("saved_subjectivity_Classifier_bert")    # Save tokenizer

# ‚úÖ Save LabelEncoder
import pickle
with open("label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import pickle

# Load the model and tokenizer from the saved directory
model = AutoModelForSequenceClassification.from_pretrained("saved_subjectivity_Classifier_bert")
tokenizer = AutoTokenizer.from_pretrained("saved_subjectivity_Classifier_bert")

# Load the LabelEncoder
with open('label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)

import torch

# üßº 1. Detection Function
def detect_label(text, lang):
    # Clean the input
    cleaned_text = clean_statement(text, lang)

    # Tokenize the input
    inputs = tokenizer(
        cleaned_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )

    # Run the model
    with torch.no_grad():
        outputs = model(**inputs)

    # Get predicted class
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()

    # Decode predicted label
    return label_encoder.inverse_transform([predicted_class])[0]

# üß™ 2. Sample Texts with Language
sample_texts = [
    ("ÿ£ÿ¥ÿπÿ± ÿ£ŸÜ ÿßŸÑŸÖÿØŸäŸÜÿ© ÿ£ÿµÿ®ÿ≠ÿ™ ÿ£ŸÉÿ´ÿ± ÿßÿ≤ÿØÿ≠ÿßŸÖŸãÿß ŸÖŸÖÿß ŸÉÿßŸÜÿ™ ÿπŸÑŸäŸá.", 'ar'),
    ("ÿ™ÿ£ÿ≥ÿ≥ÿ™ ÿ¥ÿ±ŸÉÿ© ÿ£ÿ±ÿßŸÖŸÉŸà ÿßŸÑÿ≥ÿπŸàÿØŸäÿ© ÿπÿßŸÖ 1933.", 'ar'),
    ("ÿ®ÿµÿ±ÿßÿ≠ÿ©ÿå ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠Ÿä ŸáŸà ÿßŸÑÿ£ÿ¨ŸÖŸÑ ŸÅŸä ÿßŸÑÿ±Ÿäÿßÿ∂.", 'ar'),
    ("ÿ£ÿπŸÑŸÜÿ™ ÿßŸÑÿ≠ŸÉŸàŸÖÿ© ÿ±ŸÅÿπ ÿ£ÿ≥ÿπÿßÿ± ÿßŸÑŸÉŸáÿ±ÿ®ÿßÿ° ÿ®ŸÜÿ≥ÿ®ÿ© 10Ÿ™.", 'ar'),
    ("ŸÅŸä ÿ±ÿ£ŸäŸäÿå Ÿäÿ¨ÿ® ÿ™ÿ≠ÿ≥ŸäŸÜ Ÿàÿ≥ÿßÿ¶ŸÑ ÿßŸÑŸÜŸÇŸÑ ÿßŸÑÿπÿßŸÖ.", 'ar'),
    ("ÿßŸÜÿ™ŸáŸâ ŸÖÿ¥ÿ±Ÿàÿπ ÿ™Ÿàÿ≥ÿπÿ© ÿßŸÑŸÖÿ∑ÿßÿ± ÿÆŸÑÿßŸÑ ÿπÿßŸÖ 2022.", 'ar'),
    ("ÿ£ÿ∏ŸÜ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑŸÅÿ±ŸäŸÇ ŸÑÿØŸäŸá ŸÅÿ±ÿµÿ© ŸÉÿ®Ÿäÿ±ÿ© ŸÑŸÑŸÅŸàÿ≤.", 'ar'),
    ("ÿπÿØÿØ ÿ≥ŸÉÿßŸÜ ÿßŸÑŸÖŸÖŸÑŸÉÿ© ÿ®ŸÑÿ∫ 36 ŸÖŸÑŸäŸàŸÜ ŸÜÿ≥ŸÖÿ© ÿπÿßŸÖ 2024.", 'ar'),
    ("ÿ£ÿ≠ÿ®ÿ®ÿ™ ÿ∑ÿ±ŸäŸÇÿ© ÿ™ŸÇÿØŸäŸÖ ÿßŸÑŸÖÿ≠ÿßÿ∂ÿ±ÿ© ÿ®ÿ¥ŸÉŸÑ ÿ®ÿ≥Ÿäÿ∑ ŸàŸàÿßÿ∂ÿ≠.", 'ar'),
    ("Ÿáÿ∑ŸÑÿ™ ÿ£ŸÖÿ∑ÿßÿ± ÿ∫ÿ≤Ÿäÿ±ÿ© ÿπŸÑŸâ ÿ¨ÿØÿ© ÿßŸÑÿ£ÿ≥ÿ®Ÿàÿπ ÿßŸÑŸÖÿßÿ∂Ÿä.", 'ar'),

    ("Credo che questa sia stata la scelta migliore.", 'it'),
    ("Il museo nazionale √® stato fondato nel 1865.", 'it'),
    ("Secondo me, il nuovo ristorante offre piatti deliziosi.", 'it'),
    ("La temperatura a Roma oggi √® di 27 gradi.", 'it'),
    ("Penso che dovrebbero migliorare il servizio clienti.", 'it'),
    ("L'azienda ha registrato una crescita del 12% quest'anno.", 'it'),
    ("A mio avviso, il film era troppo lungo e noioso.", 'it'),
    ("Il treno delle 10:00 per Milano √® stato cancellato.", 'it'),
    ("Mi sembra che questa decisione sia affrettata.", 'it'),
    ("Il concerto inizia alle 20:30 al teatro centrale.", 'it'),

    ("–ú–∏—Å–ª—è, —á–µ —Ç–æ–∑–∏ –ø—Ä–æ–µ–∫—Ç —â–µ –∏–º–∞ –≥–æ–ª—è–º —É—Å–ø–µ—Ö.", 'bg'),
    ("–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ—Ç–æ –Ω–∞ –∑–¥—Ä–∞–≤–µ–æ–ø–∞–∑–≤–∞–Ω–µ—Ç–æ –æ–±—è–≤–∏ –Ω–æ–≤–∞ –∫–∞–º–ø–∞–Ω–∏—è.", 'bg'),
    ("–û—Ç –º–æ—è –≥–ª–µ–¥–Ω–∞ —Ç–æ—á–∫–∞, –º–µ—Ä–∫–∏—Ç–µ —Å–∞ –ø—Ä–µ–∫–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–≥–∏.", 'bg'),
    ("–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∏—Ç–µ —â–µ –¥–æ—Å—Ç–∏–≥–Ω–∞—Ç 35 –≥—Ä–∞–¥—É—Å–∞ —Ç–∞–∑–∏ —Å–µ–¥–º–∏—Ü–∞.", 'bg'),
    ("–í—è—Ä–≤–∞–º, —á–µ –º–æ–∂–µ–º –¥–∞ –ø–æ—Å—Ç–∏–≥–Ω–µ–º –æ—â–µ –ø–æ–≤–µ—á–µ.", 'bg'),
    ("–ù–æ–≤–∏—è—Ç –º–æ—Å—Ç —â–µ –±—ä–¥–µ –∑–∞–≤—ä—Ä—à–µ–Ω –¥–æ –∫—Ä–∞—è –Ω–∞ –≥–æ–¥–∏–Ω–∞—Ç–∞.", 'bg'),
    ("–°–º—è—Ç–∞–º, —á–µ —Ç—Ä—è–±–≤–∞ –¥–∞ —Å–µ –∏–Ω–≤–µ—Å—Ç–∏—Ä–∞ –ø–æ–≤–µ—á–µ –≤ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ—Ç–æ.", 'bg'),
    ("–ù–ê–°–ê –ø–ª–∞–Ω–∏—Ä–∞ –Ω–æ–≤–∞ –º–∏—Å–∏—è –∫—ä–º –ú–∞—Ä—Å –ø—Ä–µ–∑ 2026 –≥–æ–¥–∏–Ω–∞.", 'bg'),
    ("–ß–µ—Å—Ç–Ω–æ –∫–∞–∑–∞–Ω–æ, —Ç–æ–≤–∞ –µ –Ω–∞–π-–¥–æ–±—Ä–∏—è—Ç –æ—Ç–±–æ—Ä, –∫–æ–π—Ç–æ —Å—ä–º –≤–∏–∂–¥–∞–ª.", 'bg'),
    ("–ë—ä–ª–≥–∞—Ä–∏—è —â–µ –±—ä–¥–µ –¥–æ–º–∞–∫–∏–Ω –Ω–∞ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–µ–Ω —Ñ–æ—Ä—É–º —Ç–∞–∑–∏ –µ—Å–µ–Ω.", 'bg'),

    ("I honestly believe this city is the best in the country.", 'en'),
    ("The bridge collapsed during a storm last year.", 'en'),
    ("In my opinion, the new rules are too strict.", 'en'),
    ("The hospital admitted over 300 patients last week.", 'en'),
    ("I feel this movie deserved more awards.", 'en'),
    ("The team won the championship by 5 points.", 'en'),
    ("It's clear to me that the project is a failure.", 'en'),
    ("The company reported a 7% increase in profits.", 'en'),
    ("I absolutely loved the performance last night.", 'en'),
    ("The annual conference will be held in Chicago this year.", 'en'),

    ("Meiner Meinung nach ist dieses Buch wirklich inspirierend.", 'de'),
    ("Das Museum wurde im Jahr 1902 er√∂ffnet.", 'de'),
    ("Ich finde, dass die neuen Regeln unfair sind.", 'de'),
    ("Die Temperaturen erreichten gestern 30 Grad Celsius.", 'de'),
    ("Ich glaube, dass wir mehr in Bildung investieren sollten.", 'de'),
    ("Das Unternehmen er√∂ffnete eine neue Filiale in Hamburg.", 'de'),
    ("Aus meiner Sicht war das Konzert ein voller Erfolg.", 'de'),
    ("Die Bahnstrecke wurde wegen Wartungsarbeiten gesperrt.", 'de'),
    ("Ich denke, der neue Film ist absolut sehenswert.", 'de'),
    ("Die Stadtbibliothek wird im n√§chsten Monat renoviert.", 'de')
]

# üîç 3. Run detection
for text, lang in sample_texts:
    predicted_class = detect_label(text, lang)
    print(f"üìù Sentence: {text}\n‚û°Ô∏è Predicted Label: {predicted_class}\n")

import pandas as pd
import torch
import zipfile
# import re
# import nltk
# from nltk.corpus import stopwords

# üßº 1. Detection Function



# # Download NLTK stopwords
# nltk.download('stopwords')

# # Get English stopwords from NLTK
# stop_words = set(stopwords.words('romanian'))

# def clean_statement(statement):
#     # Convert to lowercase
#     statement = statement.lower()

#     # Remove special characters (punctuation, non-alphabetic characters)
#     statement = re.sub(r'[^\w\s]', '', statement)

#     # Remove numbers
#     statement = re.sub(r'\d+', '', statement)

#     # Tokenize and remove stopwords
#     words = statement.split()
#     words = [word for word in words if word not in stop_words]

#     # Rejoin into a cleaned statement
#     cleaned_statement = ' '.join(words)
#     return cleaned_statement

# Apply to the correct column: 'sentence'


def detect_label(text):
    # cleaned_text = clean_statement(text)

    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )

    with torch.no_grad():
        outputs = model(**inputs)

    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()

    return label_encoder.inverse_transform([predicted_class])[0]

# üìÇ 2. Load the input TSV file
input_file = "/content/test_pol_unlabeled.tsv"
df = pd.read_csv(input_file, sep="\t")

assert 'sentence_id' in df.columns and 'sentence' in df.columns, "Input file must have 'sentence_id' and 'sentence' columns"

# üîç 3. Predict labels and keep alignment
labels = df['sentence'].map(detect_label)  # map() keeps index alignment

# Combine sentence_id + label
output_df = pd.DataFrame({
    'sentence_id': df['sentence_id'],
    'label': labels
})

# üíæ 4. Save the TSV output
tsv_output_path = "/content/task1_test_pol.tsv"
output_df.to_csv(tsv_output_path, sep="\t", index=False)

# üóúÔ∏è 5. Zip the TSV file
zip_output_path = "/content/task1_test_pol.zip"
with zipfile.ZipFile(zip_output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
    zipf.write(tsv_output_path, arcname="task1_test_pol.tsv")

print(f"‚úÖ TSV saved to: {tsv_output_path}")
print(f"‚úÖ ZIP file created: {zip_output_path}")